{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maheit\\Desktop\\book\\Book_recommendation_system\\book\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "import pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': 'Summarize: Notes from Underground (pre-reform Russian: –ó–∞–ø–∏—Å–∫–∏ –∏–∑—ä –ø–æ–¥–ø–æ–ª—å—è; post-reform Russian: –ó–∞–ø–∏—Å–∫–∏ –∏–∑ –ø–æ–¥–ø–æ–ª—å—è, tr. Zap√≠ski iz podp√≥l πya), also translated as Notes from the Underground or Letters from the Underworld, is an 1864 novella by Fyodor Dostoevsky. Notes is considered by many to be one of the first existentialist novels. It presents itself as an excerpt from the rambling memoirs of a bitter, isolated, unnamed narrator (generally referred to by critics as the Underground Man), who is a retired civil servant living in St. Petersburg. The first part of the story is told in monologue form, or the underground man\\'s diary, and attacks emerging Western philosophy, especially Nikolay Chernyshevsky\\'s What Is to Be Done? The second part of the book is called \"Apropos of the Wet Snow\" and describes certain events that appear to be destroying and sometimes renewing the underground man, who acts as a first person, unreliable narrator and anti-hero.', 'target_text': 'Classics, History, Literary Fiction, Philosophical'}\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "num_labels = len(mlb.classes_)  # Number of unique tropes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"KamilAin/bart-base-booksum\", num_labels=num_labels)\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory for model checkpoints\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
    "    learning_rate=2e-5,  # Standard LR for transformers\n",
    "    per_device_train_batch_size=8,  # Reduce if you face memory issues\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,  # Adjust based on dataset size\n",
    "    weight_decay=0.01,  # Regularization to prevent overfitting\n",
    "    load_best_model_at_end=True,  # Load the best model (lowest eval loss)\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\maheit\\AppData\\Local\\Temp\\ipykernel_3420\\147608070.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2552' max='2552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2552/2552 4:00:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.027700</td>\n",
       "      <td>0.091542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.078617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.071428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.068577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart_booksum_tropes\\\\tokenizer_config.json',\n",
       " './bart_booksum_tropes\\\\special_tokens_map.json',\n",
       " './bart_booksum_tropes\\\\vocab.json',\n",
       " './bart_booksum_tropes\\\\merges.txt',\n",
       " './bart_booksum_tropes\\\\added_tokens.json',\n",
       " './bart_booksum_tropes\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"KamilAin/bart-base-booksum\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart_booksum_tropes\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,  # Ideally, use a separate validation dataset\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./bart_booksum_tropes\")\n",
    "tokenizer.save_pretrained(\"./bart_booksum_tropes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tropes: Fantasy, Young Adult, Adventure, Science fiction, War, Dystopian\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load fine-tuned model\n",
    "model_path = \"./bart_booksum_tropes\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Input new book description\n",
    "book_description = \"James Anderson had a plan. Or half of one. All that matters is that he managed to do what his older brother, the famous Aaron Warner Anderson, never did: infiltrate Ark Island, the last refuge of The Reestablishment. In the past decade no outsider has breached the stronghold of the authoritarian regime, but James is in. In a prison cell, sure, but as far as James is concerned, a win is a win. It‚Äôs been ten years since the fall of The Reestablishment. Ten years since the notorious duo ‚Äî Juliette Ferrars and Aaron Warner Anderson ‚Äî led a worldwide rebellion and established the New Republic of the West. But after a decade of unsettling quiet, The Reestablishment is ready to make a devastating move, and they have the perfect person for the job.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(f\"Summarize: {book_description}\", return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs, max_length=128)\n",
    "tropes = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Predicted Tropes:\", tropes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': np.float64(0.125), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.125), 'rougeLsum': np.float64(0.125)}\n",
      "BLEU Score: {'bleu': 0.0, 'precisions': [0.375, 0.14285714285714285, 0.0, 0.0], 'brevity_penalty': 0.8824969025845955, 'length_ratio': 0.8888888888888888, 'translation_length': 8, 'reference_length': 9}\n",
      "Precision: 0.5\n",
      "Recall: 0.5\n",
      "F1 Score: 0.5\n",
      "Jaccard Score: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = \"./bart_booksum_tropes\"  # Ensure this is the correct path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Load ROUGE and BLEU metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Sample test dataset (replace with your actual test cases)\n",
    "test_cases = [\n",
    "    {\n",
    "        \"description\": \"Paedyn and Kai are reunited but face a terrible decision in this thrilling conclusion to the New York Times bestselling romantic fantasy trilogy perfect for fans of Sarah J. Maas and The Red Queen. Paedyn Gray and Kai Azer return to the Kingdom of Ilya‚Ä¶ And Paedyn has a life-altering choice to make. Whatever she decides will determine her fate‚Äîand the fate of those around her‚Äîforever. In the ultimate battle of love and loyalty, who wins?\",\n",
    "        \"expected_tropes\": \"Fantasy, Dystopia, Romance\"\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"From the #1 New York Times bestselling author of Just for the Summer comes a new playful yet deeply emotional contemporary romance.There might be no such a thing as a perfect guy, but Xavier Rush comes disastrously close. A gorgeous veterinarian giving Greek god vibes‚Äîall while cuddling a tiny kitten? Immediately yes. That is until Xavier opens his mouth and proves that even sculpted gods can say the absolute wrong thing. Like, really wrong. Of course, there‚Äôs nothing Samantha loves more than proving an asshole wrong‚Ä¶. . . unless, of course, he can admit he made a mistake. But after one incredible and seemingly endless date‚Äîpossibly the best in living history‚ÄîSamantha is forced to admit the truth, that her family is in crisis and any kind of relationship would be impossible. Samantha begs Xavier to forget her. To remember their night together as a perfect moment, as crushing as that may be.Only no amount of distance or time is nearly enough to forget that something between them. And the only thing better than one single perfect memory is to make a life‚Äîand even a love‚Äîworth remembering.\",\n",
    "        \"expected_tropes\": \"Romance, Contemporary Romance\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store predictions & references\n",
    "predictions = []\n",
    "references = []\n",
    "all_pred_labels = []\n",
    "all_true_labels = []\n",
    "\n",
    "\n",
    "\n",
    "# confusion metric \n",
    "\n",
    "all_pred_tropes_set = set()\n",
    "all_true_tropes_set = set()\n",
    "\n",
    "for case in test_cases:\n",
    "    # Tokenize input description\n",
    "    inputs = tokenizer(f\"Summarize: {case['description']}\", return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "    # Decode output\n",
    "    predicted_tropes = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Store results\n",
    "    predictions.append(predicted_tropes)\n",
    "    references.append([case[\"expected_tropes\"]])  # BLEU expects list of references\n",
    "\n",
    "    # Convert expected and predicted tropes into sets\n",
    "    pred_tropes_set = set(predicted_tropes.lower().split(\", \"))\n",
    "    true_tropes_set = set(case[\"expected_tropes\"].lower().split(\", \"))\n",
    "\n",
    "    # Merge all tropes for consistent binary encoding\n",
    "    all_pred_tropes_set.update(pred_tropes_set)\n",
    "    all_true_tropes_set.update(true_tropes_set)\n",
    "\n",
    "# Compute ROUGE\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Compute BLEU\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Convert full trope set into binary labels for P/R/F1\n",
    "unique_tropes = list(all_pred_tropes_set | all_true_tropes_set)  # Union of all tropes\n",
    "\n",
    "for case in test_cases:\n",
    "    pred_tropes_set = set(case[\"expected_tropes\"].lower().split(\", \"))\n",
    "    true_tropes_set = set(case[\"expected_tropes\"].lower().split(\", \"))\n",
    "\n",
    "    pred_binary = [1 if trope in pred_tropes_set else 0 for trope in unique_tropes]\n",
    "    true_binary = [1 if trope in true_tropes_set else 0 for trope in unique_tropes]\n",
    "\n",
    "    all_pred_labels.append(pred_binary)\n",
    "    all_true_labels.append(true_binary)\n",
    "\n",
    "# Compute Precision, Recall, and F1-score\n",
    "precision = precision_score(all_true_labels, all_pred_labels, average=\"macro\")\n",
    "recall = recall_score(all_true_labels, all_pred_labels, average=\"macro\")\n",
    "f1 = f1_score(all_true_labels, all_pred_labels, average=\"macro\")\n",
    "\n",
    "# Compute Jaccard Similarity (Better for Multi-Label Classification)\n",
    "jaccard = jaccard_score(all_true_labels, all_pred_labels, average=\"macro\")\n",
    "\n",
    "# Print results\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Jaccard Score:\", jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Count label occurrences\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m label_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mtrain_dataset\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Plot label distribution\u001b[39;00m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count label occurrences\n",
    "label_counts = np.sum(train_dataset, axis=0)\n",
    "\n",
    "# Plot label distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(len(mlb.classes_)), label_counts)\n",
    "plt.xticks(range(len(mlb.classes_)), mlb.classes_, rotation=90)\n",
    "plt.xlabel(\"Tropes\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Label Distribution in Training Data\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
