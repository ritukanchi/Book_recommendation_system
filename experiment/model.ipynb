{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "import pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': 'Summarize: Notes from Underground (pre-reform Russian: –ó–∞–ø–∏—Å–∫–∏ –∏–∑—ä –ø–æ–¥–ø–æ–ª—å—è; post-reform Russian: –ó–∞–ø–∏—Å–∫–∏ –∏–∑ –ø–æ–¥–ø–æ–ª—å—è, tr. Zap√≠ski iz podp√≥l πya), also translated as Notes from the Underground or Letters from the Underworld, is an 1864 novella by Fyodor Dostoevsky. Notes is considered by many to be one of the first existentialist novels. It presents itself as an excerpt from the rambling memoirs of a bitter, isolated, unnamed narrator (generally referred to by critics as the Underground Man), who is a retired civil servant living in St. Petersburg. The first part of the story is told in monologue form, or the underground man\\'s diary, and attacks emerging Western philosophy, especially Nikolay Chernyshevsky\\'s What Is to Be Done? The second part of the book is called \"Apropos of the Wet Snow\" and describes certain events that appear to be destroying and sometimes renewing the underground man, who acts as a first person, unreliable narrator and anti-hero.', 'target_text': 'Classics, History, Literary Fiction, Philosophical'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"new_updated_data.csv\")  # Replace with your actual file\n",
    "\n",
    "df[\"Tropes\"] = df[\"tags\"].apply(lambda x: \", \".join(x.split(\", \")) if isinstance(x, str) else \"\")\n",
    "\n",
    "data_dicts = df.apply(lambda row: {\"input_text\": f\"Summarize: {row['description']}\", \"target_text\": row[\"Tropes\"]}, axis=1).tolist()\n",
    "\n",
    "dataset = Dataset.from_list(data_dicts)\n",
    "\n",
    "print(dataset[0])  # Verify first entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5099/5099 [00:00<00:00, 6354.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KamilAin/bart-base-booksum\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(examples[\"target_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\maheit\\AppData\\Local\\Temp\\ipykernel_3420\\2928134424.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1914' max='1914' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1914/1914 3:01:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.034600</td>\n",
       "      <td>0.091972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.079885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.075368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart_booksum_tropes\\\\tokenizer_config.json',\n",
       " './bart_booksum_tropes\\\\special_tokens_map.json',\n",
       " './bart_booksum_tropes\\\\vocab.json',\n",
       " './bart_booksum_tropes\\\\merges.txt',\n",
       " './bart_booksum_tropes\\\\added_tokens.json',\n",
       " './bart_booksum_tropes\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"KamilAin/bart-base-booksum\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart_booksum_tropes\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,  # Ideally, use a separate validation dataset\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./bart_booksum_tropes\")\n",
    "tokenizer.save_pretrained(\"./bart_booksum_tropes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tropes: Fantasy, Young Adult, Adventure, Science fiction, Dystopian\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load fine-tuned model\n",
    "model_path = \"./bart_booksum_tropes\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Input new book description\n",
    "book_description = \"James Anderson had a plan. Or half of one. All that matters is that he managed to do what his older brother, the famous Aaron Warner Anderson, never did: infiltrate Ark Island, the last refuge of The Reestablishment. In the past decade no outsider has breached the stronghold of the authoritarian regime, but James is in. In a prison cell, sure, but as far as James is concerned, a win is a win. It‚Äôs been ten years since the fall of The Reestablishment. Ten years since the notorious duo ‚Äî Juliette Ferrars and Aaron Warner Anderson ‚Äî led a worldwide rebellion and established the New Republic of the West. But after a decade of unsettling quiet, The Reestablishment is ready to make a devastating move, and they have the perfect person for the job.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(f\"Summarize: {book_description}\", return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs, max_length=128)\n",
    "tropes = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Predicted Tropes:\", tropes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': np.float64(0.1), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.1), 'rougeLsum': np.float64(0.1)}\n",
      "BLEU Score: {'bleu': 0.0, 'precisions': [0.3, 0.1111111111111111, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1111111111111112, 'translation_length': 10, 'reference_length': 9}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = \"./bart_booksum_tropes\"  # Ensure this is the correct path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Load ROUGE and BLEU metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Sample test dataset (replace with your actual test cases)\n",
    "test_cases = [\n",
    "    {\n",
    "        \"description\": \"ose yourself in this exhilarating return to the #1 global bestselling Shatter Me universe, the first book in a new series set ten years after the fall of The Reestablishment. James Anderson had a plan. Or half of one. All that matters is that he managed to do what his older brother, the famous Aaron Warner Anderson, never did: infiltrate Ark Island, the last refuge of The Reestablishment. In the past decade no outsider has breached the stronghold of the authoritarian regime, but James is in. In a prison cell, sure, but as far as James is concerned, a win is a win. It‚Äôs been ten years since the fall of The Reestablishment. Ten years since the notorious duo ‚Äî Juliette Ferrars and Aaron Warner Anderson ‚Äî led a worldwide rebellion and established the New Republic of the West. But after a decade of unsettling quiet, The Reestablishment is ready to make a devastating move, and they have the perfect person for the job. Rosabelle Wolff had a plan. She always has a plan. On Ark Island, where constant surveillance is packaged as security, even emotions must be experienced with caution. A trained assassin, her every movement is monitored by synthetic intelligence‚Äîand when she‚Äôs given an order to kill, she never hesitates. Brimming with pulse-pounding action and torturous romance, Watch Me is an explosive journey through a dystopian landscape where enemies-to-lovers has never felt more impossible. Step into a beloved and breathtaking world that demands an answer to a desperate question‚Äî Who are we when no one is watching?\",\n",
    "        \"expected_tropes\": \"Fantasy, Dystopia, romance\"\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"From the #1 New York Times bestselling author of Just for the Summer comes a new playful yet deeply emotional contemporary romance. There might be no such a thing as a perfect guy, but Xavier Rush comes disastrously close. A gorgeous veterinarian giving Greek god vibes‚Äîall while cuddling a tiny kitten? Immediately yes. That is until Xavier opens his mouth and proves that even sculpted gods can say the absolute wrong thing. Like, really wrong. Of course, there‚Äôs nothing Samantha loves more than proving an asshole wrong‚Ä¶ . . . unless, of course, he can admit he made a mistake. But after one incredible and seemingly endless date‚Äîpossibly the best in living history‚ÄîSamantha is forced to admit the truth, that her family is in crisis and any kind of relationship would be impossible. Samantha begs Xavier to forget her. To remember their night together as a perfect moment, as crushing as that may be. Only no amount of distance or time is nearly enough to forget that something between them. And the only thing better than one single perfect memory is to make a life‚Äîand even a love‚Äîworth remembering.\",\n",
    "        \"expected_tropes\": \"Romance, contepmrory romance\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store predictions & references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for case in test_cases:\n",
    "    # Tokenize input description\n",
    "    inputs = tokenizer(f\"Summarize: {case['description']}\", return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128, num_beams=5)\n",
    "\n",
    "    # Decode output\n",
    "    predicted_tropes = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Store results\n",
    "    predictions.append(predicted_tropes)\n",
    "    references.append([case[\"expected_tropes\"]])  # BLEU expects list of references\n",
    "\n",
    "# Compute ROUGE\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Compute BLEU\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
