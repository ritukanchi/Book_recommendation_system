{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maheit\\dev\\book\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 3376/3376 [00:03<00:00, 876.76 examples/s]\n",
      "Map: 100%|██████████| 844/844 [00:01<00:00, 767.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Verification:\n",
      "Number of classes: 776\n",
      "Sample tags: ['Classics', 'Biography', 'Biography & Autobiography', 'Memoir', 'Language Arts & Disciplines', ' American']\n",
      "Binarized labels: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])...\n",
      "Number of active tags: 6.0\n",
      "Input shape: torch.Size([512])\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1266' max='1266' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1266/1266 2:38:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.035196</td>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.552133</td>\n",
       "      <td>0.122342</td>\n",
       "      <td>0.200301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>0.026215</td>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.552133</td>\n",
       "      <td>0.122342</td>\n",
       "      <td>0.200301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.024730</td>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.552133</td>\n",
       "      <td>0.122342</td>\n",
       "      <td>0.200301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import (\n",
    "    BartForSequenceClassification,\n",
    "    BartTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load and clean data\n",
    "df = pd.read_csv(\"new_updated_data.csv\")\n",
    "df['description'] = df['description'].fillna('').astype(str)\n",
    "df['tags'] = df['tags'].apply(lambda x: x.split(', ') if isinstance(x, str) else [])\n",
    "\n",
    "# 2. Filter out empty tags and prepare labels\n",
    "df = df[df['tags'].apply(len) > 0]  # Remove samples with no tags\n",
    "all_tags = sorted(list(set(tag for tags in df['tags'] for tag in tags)))\n",
    "num_classes = len(all_tags)\n",
    "mlb = MultiLabelBinarizer(classes=all_tags)\n",
    "\n",
    "# 3. Train-test split\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Initialize tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForSequenceClassification.from_pretrained(\n",
    "    'facebook/bart-base',\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# 5. Dataset preparation with proper label handling\n",
    "def prepare_dataset(df, mlb, is_train=False):\n",
    "    # Get binary labels\n",
    "    labels = mlb.fit_transform(df['tags']) if is_train else mlb.transform(df['tags'])\n",
    "    \n",
    "    # Convert to float32 for PyTorch\n",
    "    labels = labels.astype(np.float32)\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        'text': df['description'].tolist(),\n",
    "        'labels': labels.tolist()\n",
    "    })\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = prepare_dataset(train_df, mlb, is_train=True)\n",
    "eval_dataset = prepare_dataset(eval_df, mlb)\n",
    "\n",
    "# 6. Tokenization with proper handling\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors directly\n",
    "    )\n",
    "    \n",
    "    # Convert labels to tensor\n",
    "    tokenized['labels'] = torch.tensor(examples['labels'], dtype=torch.float)\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization with batched processing\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, batch_size=32)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True, batch_size=32)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_eval.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# 7. Verify data\n",
    "print(\"\\nData Verification:\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Sample tags: {train_df.iloc[0]['tags']}\")\n",
    "print(f\"Binarized labels: {tokenized_train[0]['labels'][:20]}...\")  # First 20 labels\n",
    "print(f\"Number of active tags: {sum(tokenized_train[0]['labels'])}\")\n",
    "print(f\"Input shape: {tokenized_train[0]['input_ids'].shape}\")  # Should be (512,)\n",
    "\n",
    "# 8. Training setup with adjusted parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=2,  # Helps with small batch sizes\n",
    "    report_to=\"none\"  # Disables wandb if not needed\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Move to CPU if needed\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    \n",
    "    # Convert to probabilities and then to binary predictions\n",
    "    preds = torch.sigmoid(torch.tensor(logits)) > 0.5\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp = (labels * preds).sum().float()\n",
    "    fp = preds.sum().float() - tp\n",
    "    fn = labels.sum().float() - tp\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall = tp / (tp + fn + 1e-10)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    accuracy = (preds == labels).float().mean()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy.item(),\n",
    "        'precision': precision.item(),\n",
    "        'recall': recall.item(),\n",
    "        'f1': f1.item()\n",
    "    }\n",
    "\n",
    "# 9. Create trainer with error handling\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # 10. Train\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 11. Save everything\n",
    "    model.save_pretrained(\"./fine_tuned_bart_book_tags\")\n",
    "    tokenizer.save_pretrained(\"./fine_tuned_bart_book_tags\")\n",
    "    import joblib\n",
    "    joblib.dump(mlb, 'label_binarizer.pkl')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    # Print more debug info if error occurs\n",
    "    print(\"\\nDebug Info:\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Sample input shape: {tokenized_train[0]['input_ids'].shape}\")\n",
    "    print(f\"Sample labels shape: {tokenized_train[0]['labels'].shape}\")\n",
    "    print(f\"Number of samples: {len(tokenized_train)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
